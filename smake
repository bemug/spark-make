#!/bin/bash

SPARK_DIR="spark-1.1.1"
SPARK_ENV="$SPARK_DIR/conf/spark-env.sh"
SPARK_SLAVES="$SPARK_DIR/conf/slaves"
SBT_FILE="simple.sbt"
SRC_DIR="src"
IP_ADDR=`ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.){3}[0-9]*).*/\2/p' | head -n 1`

#Useless funcs
function splash {
	echo "               _       "
	echo " ____ __  __ _| |_____ "
	echo "(_-< '  \/ _\` | / / -_)"
	echo "/__/_|_|_\__,_|_\_\___|"
	echo
}

function usage {
	echo "Usage:"
	echo -e "\t--compile: compile source code to be executed"
	echo -e "\t--master: configure this pc as master"
	echo -e "\t--worker <hostname>: add a worker to the worker list"
	echo -e "\t--run: run the compiled source code"
}


#Usefull funcs
function compile {
	echo "Compiling"
	#Prepare files
	cp -r $SBT_FILE $SRC_DIR $SPARK_DIR
	cd $SPARK_DIR
	#Compile
	./sbt/sbt package
	#Go back
	rm -rf $SBT_FILE $SRC_DIR
	cd ..
	echo -e "Done!\n"
}

function master {
	echo -n "Setting this computer as master.. "
	echo "export SPARK_MASTER_IP=$IP_ADDR" > $SPARK_ENV
	echo "export SPARK_MASTER_PORT=7077" >> $SPARK_ENV
	echo "export MASTER=spark://\${SPARK_MASTER_IP}:\${SPARK_MASTER_PORT}" >> $SPARK_ENV
	echo -e "Done!\n"
}

function worker {
	echo "Adding worker $WORKER_HOST"
	echo -n "Checking worker state.. "
	if ping -c 1 $WORKER_HOST &> /dev/null
	then
		echo "UP!"
		echo -n "Resolving ip.. "
		WORKER_IP=`host $WORKER_HOST | awk '/has address/ { print $4 }'`
		if [[ $WORKER_IP ]]
		then
			echo "OK"
			echo -n "Adding to workers' list.. "
			echo "$WORKER_IP" >> $SPARK_SLAVES
			echo "Done!"
		else
			echo "KO"
			echo "Unable to resolve ip, skipping"
		fi
	else
		echo "DOWN!"
		echo "Skipping $WORKER_HOST because it's down"
	fi
	echo ""
}

function clearworkers {
	echo -n "Clearing workers' list.. "
	echo "#Put here workers' ips" > $SPARK_SLAVES
	echo -e "Done!\n"
}

function run {
	echo "Run"
}

#Start
splash #Display ascii
#Check for args
if [ $# -eq 0 ]
then
    echo "Please provide an argument"
	usage
	exit -1
fi
#Check spark folder
if [ ! -d "$SPARK_DIR" ]; then
	echo "Spark directory not found, please run get-spark.sh first."
	exit -1
fi
#Parse args
while [ "$1" ]
do
	case "$1" in
	"--compile")
		compile
		;;
	"--master")
		master
		;;
	"--worker")
		shift
		WORKER_HOST=$1
		worker
		;;
	"--clear-workers")
		clearworkers
		;;
	"--run")
		run
		;;
	*)
		echo -e "\nWARNING: Unknown argument $1, skipping.\n"
		;;
	esac
    shift
done

# End of file

